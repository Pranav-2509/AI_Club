{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranav-2509/NIGHT_VISION_SELF/blob/main/Vision_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLf2iJ1koabZ"
      },
      "source": [
        "-------------------------\n",
        "#Imports\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFDL0OyGlfVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH-A-3-moZc_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydItLq63ToZN"
      },
      "source": [
        "--------------------------\n",
        "#Device Agnostic Code\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fINZTCGhTszS"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqskNl4yUK50",
        "outputId": "cdace313-a43c-4593-afb8-474a0ba4dee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print (device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4drwJr12WH1a"
      },
      "source": [
        "------------------------\n",
        "#Loading the Data\n",
        "-----------------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('train_dataset.csv')"
      ],
      "metadata": {
        "id": "rfzUANLf6nYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLKo2msj7FRz",
        "outputId": "dc5c82d4-cc47-4d44-d015-a933b4661efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id                                             pixels  emotion\n",
            "0        1  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...        0\n",
            "1        2  151 150 147 155 148 133 111 140 170 174 182 15...        0\n",
            "2        3  231 212 156 164 174 138 161 173 182 200 106 38...        2\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...        4\n",
            "4        5  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...        6\n",
            "...    ...                                                ...      ...\n",
            "4995  4996  22 24 23 23 25 24 23 20 18 13 6 2 0 1 7 22 32 ...        3\n",
            "4996  4997  73 85 87 87 74 118 120 132 134 127 133 118 105...        3\n",
            "4997  4998  253 253 254 254 254 254 250 219 166 141 109 70...        6\n",
            "4998  4999  78 84 77 95 90 85 72 75 79 84 86 82 88 102 110...        6\n",
            "4999  5000  98 100 102 104 107 109 111 119 126 130 53 5 12...        3\n",
            "\n",
            "[5000 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (train_data.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAEEpJcT8Efy",
        "outputId": "a26d58f7-c80b-4025-c468-660dc92c0f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id          int64\n",
            "pixels     object\n",
            "emotion     int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values_list = train_data.iloc [0, 1]\n",
        "values_list = values_list.split()"
      ],
      "metadata": {
        "id": "tFLiw8yi8gE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values_list2 = [float (x) for x in values_list]\n",
        "values_list2 = np.array (values_list2)\n",
        "print (values_list2.reshape ((1, 48, 48)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTaA8PB69IbJ",
        "outputId": "6ab45a0a-f5ea-4db5-967f-f1f8d97cfd5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 70.  80.  82. ...  52.  43.  41.]\n",
            "  [ 65.  61.  58. ...  56.  52.  44.]\n",
            "  [ 50.  43.  54. ...  49.  56.  47.]\n",
            "  ...\n",
            "  [ 91.  65.  42. ...  72.  56.  43.]\n",
            "  [ 77.  82.  79. ... 105.  70.  46.]\n",
            "  [ 77.  72.  84. ... 106. 109.  82.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting the data into a usable format"
      ],
      "metadata": {
        "id": "wpfh6nSEB0dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_tuples = []"
      ],
      "metadata": {
        "id": "_7ApNC63DnQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (5000):\n",
        "  pixel_vals = train_data.iloc [i, 1]\n",
        "  pixel_vals = pixel_vals.split()\n",
        "  pixel_vals = [float (x) for x in pixel_vals]\n",
        "  pixel_vals = [x/255.0 for x in pixel_vals]\n",
        "  pixel_vals = [(x - 0.5) / 0.5 for x in pixel_vals]\n",
        "  pixel_vals = np.array (pixel_vals)\n",
        "  pixel_vals = pixel_vals.reshape ((1, 48, 48))\n",
        "  pixel_vals = torch.from_numpy (pixel_vals)\n",
        "  label = train_data.iloc [i, 2]\n",
        "  pair = (pixel_vals, label)\n",
        "  train_data_tuples.append (pair)"
      ],
      "metadata": {
        "id": "28cZDyWx_fqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_tuples[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8zzE9GdDzYZ",
        "outputId": "af08a5c6-3ade-4a3d-913e-4087610b60ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-0.4510, -0.3725, -0.3569,  ..., -0.5922, -0.6627, -0.6784],\n",
              "          [-0.4902, -0.5216, -0.5451,  ..., -0.5608, -0.5922, -0.6549],\n",
              "          [-0.6078, -0.6627, -0.5765,  ..., -0.6157, -0.5608, -0.6314],\n",
              "          ...,\n",
              "          [-0.2863, -0.4902, -0.6706,  ..., -0.4353, -0.5608, -0.6627],\n",
              "          [-0.3961, -0.3569, -0.3804,  ..., -0.1765, -0.4510, -0.6392],\n",
              "          [-0.3961, -0.4353, -0.3412,  ..., -0.1686, -0.1451, -0.3569]]],\n",
              "        dtype=torch.float64),\n",
              " 0)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = train_data_tuples[:4000]\n",
        "validation_data = train_data_tuples[4000:5000]"
      ],
      "metadata": {
        "id": "myMVXxb8HGMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torchvision import datasets, transforms\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# # Define a transform to normalize the data\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5,), (0.5,))\n",
        "# ])\n",
        "\n",
        "\n",
        "# features = torch.tensor(train_data.iloc[:, :-1].values, dtype=torch.float32)\n",
        "# # Assuming the last column is the target\n",
        "# targets = torch.tensor(train_data.iloc[:, -1].values, dtype=torch.float32)\n",
        "\n",
        "# train_data = TensorDataset(features, targets)\n",
        "# train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# # # Load the MNIST dataset\n",
        "# # train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "# # test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# # # Define batch size\n",
        "# # batch_size = 64\n",
        "\n",
        "# # print (type(train_dataset))\n",
        "# # # Create DataLoaders for training and test sets\n",
        "# # trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# # testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Verify the shape of the batches\n",
        "# for images, labels in train_loader:\n",
        "#     print(images.shape, labels.shape)\n",
        "#     break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "AdAuKh9740OT",
        "outputId": "c43a18c1-4b5f-433a-d85f-3b044e415853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6fd5369f1b23>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# Assuming the last column is the target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataloader creation\n",
        "\n",
        "train_loader = DataLoader (training_data, batch_size = 64, shuffle = True)\n",
        "test_loader = DataLoader (validation_data, batch_size = 64, shuffle = True)"
      ],
      "metadata": {
        "id": "Y708ahZtHq4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch, (data, label) in enumerate (train_loader):\n",
        "  print (data.shape)\n",
        "  print (label.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hUI1hKvIHxV",
        "outputId": "225e9187-c1e6-4a6d-ad4a-de96ca9f6630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 48, 48])\n",
            "torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 7"
      ],
      "metadata": {
        "id": "bokB87Wm4zor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the CIFAR - 100 dataset"
      ],
      "metadata": {
        "id": "NYetU65j6BBv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQIj3aelWKPA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6305b593-cb2e-467f-cf24-990dabf36f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Loaded CIFAR-10 dataset\n",
            "Batch of images shape: torch.Size([128, 3, 32, 32])\n",
            "Batch of labels shape: torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5 ,0.5, 0.5))\n",
        "])\n",
        "batch_size = 128\n",
        "\n",
        "# Download and load the CIFAR-100 training set\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# Download and load the CIFAR-100 test set\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Classes in CIFAR-100\n",
        "classes = trainset.classes\n",
        "num_classes = len (classes)\n",
        "\n",
        "# Example to iterate through the training set\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "print('Loaded CIFAR-10 dataset')\n",
        "print('Batch of images shape:', images.shape)\n",
        "print('Batch of labels shape:', labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrd3MW4WoiIM"
      },
      "source": [
        "-------------------------\n",
        "#Splitting image into tokens\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTaMij8dumwL"
      },
      "source": [
        "# Patch Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVF3ulMkFnsR"
      },
      "outputs": [],
      "source": [
        "patch_size = 8\n",
        "image_dim = images.shape[-1]\n",
        "token_length = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPkuHmSeuqPQ"
      },
      "outputs": [],
      "source": [
        "class Patch_Tokenization (nn.Module):\n",
        "  def __init__ (self, img_size, patch_size : int = 50, token_length : int = 768):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.img_size = img_size\n",
        "    c, h, w = self.img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.token_length = token_length\n",
        "\n",
        "    self.no_tokens = (h*w)/(patch_size**2)\n",
        "\n",
        "    #Layers\n",
        "    self.split = nn.Unfold (kernel_size = self.patch_size,stride = self.patch_size, padding = 0)\n",
        "    self.linear = nn.Linear (in_features = c * (self.patch_size)**2, out_features = self.token_length)\n",
        "\n",
        "\n",
        "  def forward (self, img):\n",
        "    img = self.split (img).transpose(1, 2)\n",
        "    img =  img.to(dtype=torch.float32)\n",
        "    img = self.linear (img)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2SmugCrsdAB"
      },
      "outputs": [],
      "source": [
        "tokeniser = Patch_Tokenization (images[0].shape, patch_size = patch_size, token_length = token_length)\n",
        "tokeniser = tokeniser.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI2Q582IFW2U"
      },
      "outputs": [],
      "source": [
        "num_tokens = int ((image_dim/patch_size )**2)       #If its not int, then it will be float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzXHZe2jzT4k"
      },
      "source": [
        "--------------------\n",
        "#Position Encoding\n",
        "-------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VNxZ_FE0oD1"
      },
      "outputs": [],
      "source": [
        "def position_embedding (num_tokens : int, token_length : int):\n",
        "  positions = torch.zeros ((num_tokens, token_length))\n",
        "  for i in range (num_tokens):\n",
        "    for j in range (token_length):\n",
        "      angle = i/ (np.power(1000, (2* (j//2))/token_length))\n",
        "      if (j%2==0):\n",
        "        positions[i][j] = np.sin (angle)\n",
        "      else:\n",
        "        positions[i][j] = np.cos (angle)\n",
        "\n",
        "  return positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB0zlx_K2LVK"
      },
      "outputs": [],
      "source": [
        "position_encoding = position_embedding (num_tokens+1, token_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xcs_T_TK3QPw",
        "outputId": "f0c6571f-a0e4-4859-acb3-589a100018e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 17, 50])\n"
          ]
        }
      ],
      "source": [
        "position_encoding = position_encoding.unsqueeze (dim = 0)\n",
        "print (position_encoding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyFq34oQ9wn1",
        "outputId": "efed33a6-5f07-4d99-c67b-dff525c5a300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,\n",
            "           1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
            "           0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,\n",
            "           1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
            "           0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,\n",
            "           1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
            "           0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,\n",
            "           1.0000],\n",
            "         [ 0.8415,  0.5403,  0.6879,  0.7258,  0.5442,  0.8390,  0.4228,\n",
            "           0.9062,  0.3251,  0.9457,  0.2486,  0.9686,  0.1894,  0.9819,\n",
            "           0.1440,  0.9896,  0.1094,  0.9940,  0.0831,  0.9965,  0.0631,\n",
            "           0.9980,  0.0478,  0.9989,  0.0363,  0.9993,  0.0275,  0.9996,\n",
            "           0.0209,  0.9998,  0.0158,  0.9999,  0.0120,  0.9999,  0.0091,\n",
            "           1.0000,  0.0069,  1.0000,  0.0052,  1.0000,  0.0040,  1.0000,\n",
            "           0.0030,  1.0000,  0.0023,  1.0000,  0.0017,  1.0000,  0.0013,\n",
            "           1.0000],\n",
            "         [ 0.9093, -0.4161,  0.9986,  0.0536,  0.9131,  0.4077,  0.7663,\n",
            "           0.6425,  0.6149,  0.7886,  0.4815,  0.8764,  0.3719,  0.9283,\n",
            "           0.2851,  0.9585,  0.2175,  0.9761,  0.1656,  0.9862,  0.1259,\n",
            "           0.9920,  0.0956,  0.9954,  0.0726,  0.9974,  0.0551,  0.9985,\n",
            "           0.0418,  0.9991,  0.0317,  0.9995,  0.0240,  0.9997,  0.0182,\n",
            "           0.9998,  0.0138,  0.9999,  0.0105,  0.9999,  0.0080,  1.0000,\n",
            "           0.0060,  1.0000,  0.0046,  1.0000,  0.0035,  1.0000,  0.0026,\n",
            "           1.0000],\n",
            "         [ 0.1411, -0.9900,  0.7617, -0.6480,  0.9879, -0.1549,  0.9661,\n",
            "           0.2583,  0.8379,  0.5458,  0.6842,  0.7293,  0.5410,  0.8410,\n",
            "           0.4202,  0.9074,  0.3230,  0.9464,  0.2469,  0.9690,  0.1882,\n",
            "           0.9821,  0.1431,  0.9897,  0.1087,  0.9941,  0.0825,  0.9966,\n",
            "           0.0626,  0.9980,  0.0475,  0.9989,  0.0361,  0.9993,  0.0274,\n",
            "           0.9996,  0.0208,  0.9998,  0.0157,  0.9999,  0.0119,  0.9999,\n",
            "           0.0091,  1.0000,  0.0069,  1.0000,  0.0052,  1.0000,  0.0040,\n",
            "           1.0000],\n",
            "         [-0.7568, -0.6536,  0.1071, -0.9943,  0.7445, -0.6676,  0.9847,\n",
            "          -0.1744,  0.9698,  0.2438,  0.8440,  0.5363,  0.6905,  0.7233,\n",
            "           0.5465,  0.8375,  0.4247,  0.9054,  0.3266,  0.9452,  0.2497,\n",
            "           0.9683,  0.1903,  0.9817,  0.1447,  0.9895,  0.1099,  0.9939,\n",
            "           0.0835,  0.9965,  0.0634,  0.9980,  0.0481,  0.9988,  0.0365,\n",
            "           0.9993,  0.0277,  0.9996,  0.0210,  0.9998,  0.0159,  0.9999,\n",
            "           0.0121,  0.9999,  0.0092,  1.0000,  0.0070,  1.0000,  0.0053,\n",
            "           1.0000],\n",
            "         [-0.9589,  0.2837, -0.6062, -0.7953,  0.2613, -0.9653,  0.8186,\n",
            "          -0.5743,  0.9964, -0.0848,  0.9508,  0.3097,  0.8150,  0.5795,\n",
            "           0.6614,  0.7500,  0.5212,  0.8534,  0.4040,  0.9148,  0.3103,\n",
            "           0.9506,  0.2370,  0.9715,  0.1805,  0.9836,  0.1373,  0.9905,\n",
            "           0.1043,  0.9945,  0.0792,  0.9969,  0.0601,  0.9982,  0.0456,\n",
            "           0.9990,  0.0346,  0.9994,  0.0262,  0.9997,  0.0199,  0.9998,\n",
            "           0.0151,  0.9999,  0.0115,  0.9999,  0.0087,  1.0000,  0.0066,\n",
            "           1.0000],\n",
            "         [-0.2794,  0.9602, -0.9871, -0.1602, -0.3061, -0.9520,  0.4990,\n",
            "          -0.8666,  0.9147, -0.4041,  0.9980,  0.0636,  0.9100,  0.4146,\n",
            "           0.7626,  0.6469,  0.6114,  0.7913,  0.4786,  0.8780,  0.3696,\n",
            "           0.9292,  0.2832,  0.9590,  0.2161,  0.9764,  0.1645,  0.9864,\n",
            "           0.1250,  0.9922,  0.0950,  0.9955,  0.0721,  0.9974,  0.0547,\n",
            "           0.9985,  0.0415,  0.9991,  0.0315,  0.9995,  0.0239,  0.9997,\n",
            "           0.0181,  0.9998,  0.0137,  0.9999,  0.0104,  0.9999,  0.0079,\n",
            "           1.0000],\n",
            "         [ 0.6570,  0.7539, -0.8267,  0.5627, -0.7749, -0.6321,  0.0859,\n",
            "          -0.9963,  0.7336, -0.6795,  0.9825, -0.1864,  0.9721,  0.2348,\n",
            "           0.8478,  0.5303,  0.6944,  0.7196,  0.5499,  0.8352,  0.4274,\n",
            "           0.9040,  0.3288,  0.9444,  0.2514,  0.9679,  0.1916,  0.9815,\n",
            "           0.1457,  0.9893,  0.1107,  0.9939,  0.0841,  0.9965,  0.0638,\n",
            "           0.9980,  0.0484,  0.9988,  0.0367,  0.9993,  0.0279,  0.9996,\n",
            "           0.0211,  0.9998,  0.0160,  0.9999,  0.0122,  0.9999,  0.0092,\n",
            "           1.0000],\n",
            "         [ 0.9894, -0.1455, -0.2129,  0.9771, -0.9941, -0.1087, -0.3434,\n",
            "          -0.9392,  0.4729, -0.8811,  0.9053, -0.4248,  0.9989,  0.0464,\n",
            "           0.9153,  0.4027,  0.7689,  0.6393,  0.6174,  0.7867,  0.4836,\n",
            "           0.8753,  0.3736,  0.9276,  0.2864,  0.9581,  0.2186,  0.9758,\n",
            "           0.1664,  0.9861,  0.1265,  0.9920,  0.0960,  0.9954,  0.0729,\n",
            "           0.9973,  0.0553,  0.9985,  0.0420,  0.9991,  0.0318,  0.9995,\n",
            "           0.0242,  0.9997,  0.0183,  0.9998,  0.0139,  0.9999,  0.0105,\n",
            "           0.9999],\n",
            "         [ 0.4121, -0.9111,  0.5176,  0.8556, -0.8931,  0.4498, -0.7083,\n",
            "          -0.7059,  0.1607, -0.9870,  0.7713, -0.6365,  0.9896, -0.1436,\n",
            "           0.9638,  0.2666,  0.8343,  0.5513,  0.6806,  0.7327,  0.5378,\n",
            "           0.8431,  0.4176,  0.9086,  0.3210,  0.9471,  0.2453,  0.9694,\n",
            "           0.1869,  0.9824,  0.1422,  0.9898,  0.1080,  0.9942,  0.0820,\n",
            "           0.9966,  0.0622,  0.9981,  0.0472,  0.9989,  0.0358,  0.9994,\n",
            "           0.0272,  0.9996,  0.0206,  0.9998,  0.0156,  0.9999,  0.0119,\n",
            "           0.9999],\n",
            "         [-0.5440, -0.8391,  0.9642,  0.2650, -0.5045,  0.8634, -0.9403,\n",
            "          -0.3403, -0.1689, -0.9856,  0.5889, -0.8082,  0.9445, -0.3285,\n",
            "           0.9922,  0.1250,  0.8896,  0.4567,  0.7391,  0.6736,  0.5899,\n",
            "           0.8075,  0.4606,  0.8876,  0.3552,  0.9348,  0.2720,  0.9623,\n",
            "           0.2074,  0.9783,  0.1578,  0.9875,  0.1199,  0.9928,  0.0911,\n",
            "           0.9958,  0.0691,  0.9976,  0.0525,  0.9986,  0.0398,  0.9992,\n",
            "           0.0302,  0.9995,  0.0229,  0.9997,  0.0174,  0.9998,  0.0132,\n",
            "           0.9999],\n",
            "         [-1.0000,  0.0044,  0.8822, -0.4710,  0.0466,  0.9989, -0.9960,\n",
            "           0.0892, -0.4802, -0.8772,  0.3695, -0.9292,  0.8652, -0.5014,\n",
            "           0.9998, -0.0192,  0.9342,  0.3566,  0.7925,  0.6098,  0.6397,\n",
            "           0.7687,  0.5025,  0.8646,  0.3889,  0.9213,  0.2984,  0.9545,\n",
            "           0.2278,  0.9737,  0.1735,  0.9848,  0.1319,  0.9913,  0.1002,\n",
            "           0.9950,  0.0760,  0.9971,  0.0577,  0.9983,  0.0438,  0.9990,\n",
            "           0.0332,  0.9994,  0.0252,  0.9997,  0.0191,  0.9998,  0.0145,\n",
            "           0.9999],\n",
            "         [-0.5366,  0.8439,  0.3163, -0.9487,  0.5827,  0.8127, -0.8649,\n",
            "           0.5019, -0.7393, -0.6734,  0.1270, -0.9919,  0.7546, -0.6562,\n",
            "           0.9866, -0.1630,  0.9677,  0.2523,  0.8405,  0.5419,  0.6869,\n",
            "           0.7268,  0.5433,  0.8395,  0.4220,  0.9066,  0.3245,  0.9459,\n",
            "           0.2481,  0.9687,  0.1890,  0.9820,  0.1438,  0.9896,  0.1092,\n",
            "           0.9940,  0.0829,  0.9966,  0.0629,  0.9980,  0.0478,  0.9989,\n",
            "           0.0362,  0.9993,  0.0275,  0.9996,  0.0209,  0.9998,  0.0158,\n",
            "           0.9999],\n",
            "         [ 0.4202,  0.9074, -0.4230, -0.9061,  0.9311,  0.3647, -0.5716,\n",
            "           0.8205, -0.9180, -0.3965, -0.1235, -0.9923,  0.6167, -0.7872,\n",
            "           0.9529, -0.3034,  0.9895,  0.1449,  0.8826,  0.4702,  0.7313,\n",
            "           0.6820,  0.5828,  0.8126,  0.4547,  0.8907,  0.3504,  0.9366,\n",
            "           0.2683,  0.9633,  0.2046,  0.9788,  0.1557,  0.9878,  0.1183,\n",
            "           0.9930,  0.0898,  0.9960,  0.0682,  0.9977,  0.0517,  0.9987,\n",
            "           0.0392,  0.9992,  0.0298,  0.9996,  0.0226,  0.9997,  0.0171,\n",
            "           0.9999],\n",
            "         [ 0.9906,  0.1367, -0.9303, -0.3667,  0.9796, -0.2008, -0.1711,\n",
            "           0.9853, -0.9971, -0.0765, -0.3663, -0.9305,  0.4564, -0.8898,\n",
            "           0.8992, -0.4375,  0.9994,  0.0357,  0.9186,  0.3952,  0.7729,\n",
            "           0.6346,  0.6211,  0.7838,  0.4867,  0.8736,  0.3761,  0.9266,\n",
            "           0.2883,  0.9575,  0.2201,  0.9755,  0.1675,  0.9859,  0.1273,\n",
            "           0.9919,  0.0967,  0.9953,  0.0734,  0.9973,  0.0557,  0.9984,\n",
            "           0.0423,  0.9991,  0.0321,  0.9995,  0.0243,  0.9997,  0.0185,\n",
            "           0.9998],\n",
            "         [ 0.6503, -0.7597, -0.9275,  0.3738,  0.7126, -0.7016,  0.2615,\n",
            "           0.9652, -0.9678,  0.2518, -0.5861, -0.8102,  0.2796, -0.9601,\n",
            "           0.8268, -0.5625,  0.9973, -0.0739,  0.9482,  0.3176,  0.8113,\n",
            "           0.5846,  0.6578,  0.7532,  0.5181,  0.8553,  0.4015,  0.9159,\n",
            "           0.3083,  0.9513,  0.2355,  0.9719,  0.1794,  0.9838,  0.1364,\n",
            "           0.9907,  0.1036,  0.9946,  0.0786,  0.9969,  0.0597,  0.9982,\n",
            "           0.0453,  0.9990,  0.0344,  0.9994,  0.0261,  0.9997,  0.0198,\n",
            "           0.9998],\n",
            "         [-0.2879, -0.9577, -0.4161,  0.9093,  0.2160, -0.9764,  0.6450,\n",
            "           0.7642, -0.8333,  0.5528, -0.7691, -0.6391,  0.0927, -0.9957,\n",
            "           0.7372, -0.6757,  0.9832, -0.1825,  0.9713,  0.2377,  0.8466,\n",
            "           0.5323,  0.6931,  0.7208,  0.5488,  0.8360,  0.4266,  0.9045,\n",
            "           0.3281,  0.9446,  0.2509,  0.9680,  0.1912,  0.9816,  0.1454,\n",
            "           0.9894,  0.1105,  0.9939,  0.0839,  0.9965,  0.0637,  0.9980,\n",
            "           0.0483,  0.9988,  0.0366,  0.9993,  0.0278,  0.9996,  0.0211,\n",
            "           0.9998]]])\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "print (position_encoding)\n",
        "position_encoding = position_encoding.to(device)\n",
        "print (position_encoding.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoOQ2dFfOsY_"
      },
      "source": [
        "------------------\n",
        "#Attention Block\n",
        "------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QMlePmbO9Gn"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self,\n",
        "                dim: int,\n",
        "                chan: int,\n",
        "                num_heads: int=1,\n",
        "                qkv_bias: bool=False,\n",
        "                qk_scale = None):\n",
        "\n",
        "        \"\"\" Attention Module\n",
        "\n",
        "            Args:\n",
        "                dim (int): input size of a single token\n",
        "                chan (int): resulting size of a single token after concatenating the heads\n",
        "                num_heads(int): number of attention heads in MSA\n",
        "                qkv_bias (bool): determines if the qkv layer learns an addative bias\n",
        "                qk_scale (NoneFloat): value to scale the queries and keys by;\n",
        "                                    if None, queries and keys are scaled by ``head_dim ** -0.5``\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        ## Define Constants\n",
        "        self.num_heads = num_heads\n",
        "        self.chan = chan\n",
        "        self.head_dim = self.chan // self.num_heads\n",
        "        self.scale = qk_scale or self.head_dim ** -0.5\n",
        "        assert self.chan % self.num_heads == 0, '\"Chan\" must be evenly divisible by \"num_heads\".'\n",
        "\n",
        "        ## Define Layers\n",
        "        self.layer_norm = nn.LayerNorm (dim)\n",
        "        self.q = nn.Linear(dim, chan , bias=qkv_bias)\n",
        "        self.k = nn.Linear (dim, chan, bias = qkv_bias)\n",
        "        self.v = nn.Linear (dim, chan, bias = qkv_bias)\n",
        "\n",
        "        #### Each token gets projected from starting length (dim) to channel length (chan) 3 times (for each Q, K, V)\n",
        "        self.proj = nn.Linear(chan, chan)\n",
        "\n",
        "\n",
        "\n",
        "    #     self._initialize_weights()\n",
        "\n",
        "    # def _initialize_weights(self):\n",
        "    #     for m in self.modules():\n",
        "    #         if isinstance(m, nn.Linear):\n",
        "    #             torch.nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "    #             if m.bias is not None:\n",
        "    #                 nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        ## Dimensions: (batch, num_tokens, token_len)\n",
        "\n",
        "\n",
        "        x = self.layer_norm (x)\n",
        "\n",
        "        ## Calcuate QKVs\n",
        "\n",
        "        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        #### Dimensions: (3, batch, heads, num_tokens, chan/num_heads = head_dim)\n",
        "        q_matrix, k_matrix, v_matrix = self.q (x), self.k (x), self.v (x)\n",
        "        q_matrix = q_matrix.reshape (B, self.num_heads, N, self.head_dim)\n",
        "        k_matrix = k_matrix.reshape (B, self.num_heads, N, self.head_dim)\n",
        "        v_matrix = v_matrix.reshape (B, self.num_heads, N, self.head_dim)\n",
        "\n",
        "\n",
        "        ## Calculate Attention\n",
        "        attn = (q_matrix * self.scale) @ k_matrix.transpose(-2, -1)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        #### Dimensions: (batch, heads, num_tokens, num_tokens)\n",
        "\n",
        "        ## Attention Layer\n",
        "        x = (attn @ v_matrix).reshape(B, N, self.chan)\n",
        "        #### Dimensions: (batch, heads, num_tokens, chan)\n",
        "\n",
        "        ## Projection Layers\n",
        "        x = self.proj(x)\n",
        "        ## Skip Connection Layer\n",
        "        v_matrix = v_matrix.reshape(B, N, self.chan)\n",
        "        x = v_matrix + x\n",
        "        #### Because the original x has different size with current x, use v to do skip connection\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample_input = torch.rand ((5, 17, 50))\n",
        "# e = Attention (50, 100, 2)\n",
        "# output = e (sample_input)\n",
        "# print (output.shape)"
      ],
      "metadata": {
        "id": "5FlvstR-V3rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkfxuD8iP3MN"
      },
      "source": [
        "----------------------------------\n",
        "#Neural Net at the end of the Encoder\n",
        "-----------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2Obl6eTP74o"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self,\n",
        "       in_chan: int,\n",
        "       hidden_chan = None,\n",
        "       out_chan = None,\n",
        "       act_layer = nn.GELU()):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        ## Define Number of Channels\n",
        "        hidden_chan = hidden_chan\n",
        "        out_chan = out_chan\n",
        "\n",
        "        ## Define Layers\n",
        "        self.fc1 = nn.Linear(in_chan, hidden_chan)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_chan, out_chan)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LELTfGA_fcEr"
      },
      "source": [
        "-----------------------------------\n",
        "#Probabilities extraction\n",
        "------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iufxWehnfbt2"
      },
      "outputs": [],
      "source": [
        "class probabilities (nn.Module ):\n",
        "  def __init__ (self,in_size : int, num_classes : int):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer1 = nn.Linear (in_size, 70)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.layer2 = nn.Linear (70, num_classes)\n",
        "\n",
        "  #   self._initialize_weights()\n",
        "\n",
        "  # def _initialize_weights(self):\n",
        "  #       for m in self.modules():\n",
        "  #           if isinstance(m, nn.Linear):\n",
        "  #               torch.nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "  #               if m.bias is not None:\n",
        "  #                   nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "  def forward (self, x):\n",
        "    x =  (self.layer1(x))\n",
        "    x = self.activation (x)\n",
        "    x = (self.layer2(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq-pZBL54Gzt"
      },
      "source": [
        "--------------------------------\n",
        "#Encoding Block\n",
        "--------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuF0JD3R4KGt"
      },
      "outputs": [],
      "source": [
        "class Encoding(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "       dim: int,\n",
        "       num_heads: int=1,\n",
        "       hidden_chan_mul = 4.,\n",
        "       qkv_bias = False,\n",
        "       qk_scale = None,\n",
        "       act_layer=nn.ReLU,\n",
        "       norm_layer=nn.LayerNorm):\n",
        "\n",
        "        \"\"\" Encoding Block\n",
        "\n",
        "            Args:\n",
        "                dim (int): size of a single token\n",
        "                num_heads(int): number of attention heads in MSA\n",
        "                hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet component\n",
        "                qkv_bias (bool): determines if the qkv layer learns an addative bias\n",
        "                qk_scale (NoneFloat): value to scale the queries and keys by;\n",
        "                                    if None, queries and keys are scaled by ``head_dim ** -0.5``\n",
        "                act_layer(nn.modules.activation): torch neural network layer class to use as activation\n",
        "                norm_layer(nn.modules.normalization): torch neural network layer class to use as normalization\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        ## Define Layers\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(dim=dim,\n",
        "                            chan=dim,\n",
        "                            num_heads=num_heads,\n",
        "                            qkv_bias=qkv_bias,\n",
        "                            qk_scale=qk_scale)\n",
        "        self.dropout = nn.Dropout (p = 0.5)\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        self.neuralnet = NeuralNet(in_chan=dim,\n",
        "                                hidden_chan=int(dim*hidden_chan_mul),\n",
        "                                out_chan=dim,\n",
        "                                act_layer=act_layer)\n",
        "\n",
        "        self.probabs = probabilities (dim, num_classes)\n",
        "\n",
        "    #     self._initialize_weights()\n",
        "\n",
        "    # def _initialize_weights(self):\n",
        "    #     for m in self.modules():\n",
        "    #         if isinstance(m, nn.Linear):\n",
        "    #             torch.nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "    #             if m.bias is not None:\n",
        "    #                 nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout (self.attn((x)))\n",
        "        # print ('1', x)\n",
        "        x = x + self.dropout (self.neuralnet(self.norm2(x)))\n",
        "        # print ('2', x)\n",
        "        x = x[:, 0]\n",
        "        x = self.probabs (x)\n",
        "        # print ('3', x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYXOjZ9rB9lv"
      },
      "outputs": [],
      "source": [
        "encoder  = Encoding (dim = token_length, num_heads = 2, hidden_chan_mul = 1, qkv_bias = True)\n",
        "encoder = encoder.to (device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5_yIl9FA8jS"
      },
      "source": [
        "----------------------------\n",
        "#Training Loop\n",
        "---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQbBCOb9d7Wz"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKgtZ6UgdkCz"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum = 0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zYpbJMzwA_Ur",
        "outputId": "9a2325bf-ee44-43fe-e052-76d0d9841d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH : 1\n",
            "Train Loss for batch : 1.8826812505722046\n",
            "test_loss per batch = 1.8067229986190796\n",
            "test accuracy : 0.3156\n",
            "EPOCH : 2\n",
            "Train Loss for batch : 1.8805748224258423\n",
            "test_loss per batch = 1.767155408859253\n",
            "test accuracy : 0.3345\n",
            "EPOCH : 3\n",
            "Train Loss for batch : 1.8747856616973877\n",
            "test_loss per batch = 1.7641940116882324\n",
            "test accuracy : 0.3453\n",
            "EPOCH : 4\n",
            "Train Loss for batch : 1.8778167963027954\n",
            "test_loss per batch = 1.7859960794448853\n",
            "test accuracy : 0.3255\n",
            "EPOCH : 5\n",
            "Train Loss for batch : 1.8644273281097412\n",
            "test_loss per batch = 1.7845125198364258\n",
            "test accuracy : 0.3335\n",
            "EPOCH : 6\n",
            "Train Loss for batch : 1.8698080778121948\n",
            "test_loss per batch = 1.7712483406066895\n",
            "test accuracy : 0.3278\n",
            "EPOCH : 7\n",
            "Train Loss for batch : 1.865853190422058\n",
            "test_loss per batch = 1.806781530380249\n",
            "test accuracy : 0.3326\n",
            "EPOCH : 8\n",
            "Train Loss for batch : 1.8687809705734253\n",
            "test_loss per batch = 1.82599937915802\n",
            "test accuracy : 0.3075\n",
            "EPOCH : 9\n",
            "Train Loss for batch : 1.8718289136886597\n",
            "test_loss per batch = 1.7519947290420532\n",
            "test accuracy : 0.3494\n",
            "EPOCH : 10\n",
            "Train Loss for batch : 1.85955810546875\n",
            "test_loss per batch = 1.7655887603759766\n",
            "test accuracy : 0.3472\n",
            "EPOCH : 11\n",
            "Train Loss for batch : 1.8638447523117065\n",
            "test_loss per batch = 1.8091881275177002\n",
            "test accuracy : 0.3182\n",
            "EPOCH : 12\n",
            "Train Loss for batch : 1.8593294620513916\n",
            "test_loss per batch = 1.8101999759674072\n",
            "test accuracy : 0.316\n",
            "EPOCH : 13\n",
            "Train Loss for batch : 1.8553688526153564\n",
            "test_loss per batch = 1.7908536195755005\n",
            "test accuracy : 0.3286\n",
            "EPOCH : 14\n",
            "Train Loss for batch : 1.85663902759552\n",
            "test_loss per batch = 1.743939757347107\n",
            "test accuracy : 0.3388\n",
            "EPOCH : 15\n",
            "Train Loss for batch : 1.8568408489227295\n",
            "test_loss per batch = 1.8039042949676514\n",
            "test accuracy : 0.3245\n",
            "EPOCH : 16\n",
            "Train Loss for batch : 1.8561240434646606\n",
            "test_loss per batch = 1.8272103071212769\n",
            "test accuracy : 0.2912\n",
            "EPOCH : 17\n",
            "Train Loss for batch : 1.8629387617111206\n",
            "test_loss per batch = 1.81473708152771\n",
            "test accuracy : 0.3194\n",
            "EPOCH : 18\n",
            "Train Loss for batch : 1.8570940494537354\n",
            "test_loss per batch = 1.776620864868164\n",
            "test accuracy : 0.3362\n",
            "EPOCH : 19\n",
            "Train Loss for batch : 1.8566688299179077\n",
            "test_loss per batch = 1.74354088306427\n",
            "test accuracy : 0.3466\n",
            "EPOCH : 20\n",
            "Train Loss for batch : 1.8595143556594849\n",
            "test_loss per batch = 1.7679800987243652\n",
            "test accuracy : 0.3332\n",
            "EPOCH : 21\n",
            "Train Loss for batch : 1.8610255718231201\n",
            "test_loss per batch = 1.7976219654083252\n",
            "test accuracy : 0.3257\n",
            "EPOCH : 22\n",
            "Train Loss for batch : 1.856166958808899\n",
            "test_loss per batch = 1.740057110786438\n",
            "test accuracy : 0.3455\n",
            "EPOCH : 23\n",
            "Train Loss for batch : 1.8517735004425049\n",
            "test_loss per batch = 1.8051788806915283\n",
            "test accuracy : 0.3207\n",
            "EPOCH : 24\n",
            "Train Loss for batch : 1.852439284324646\n",
            "test_loss per batch = 1.7436625957489014\n",
            "test accuracy : 0.3531\n",
            "EPOCH : 25\n",
            "Train Loss for batch : 1.859896183013916\n",
            "test_loss per batch = 1.7591700553894043\n",
            "test accuracy : 0.3466\n",
            "EPOCH : 26\n",
            "Train Loss for batch : 1.8583778142929077\n",
            "test_loss per batch = 1.7816553115844727\n",
            "test accuracy : 0.3407\n",
            "EPOCH : 27\n",
            "Train Loss for batch : 1.8498945236206055\n",
            "test_loss per batch = 1.7525901794433594\n",
            "test accuracy : 0.3474\n",
            "EPOCH : 28\n",
            "Train Loss for batch : 1.8534778356552124\n",
            "test_loss per batch = 1.7542804479599\n",
            "test accuracy : 0.3419\n",
            "EPOCH : 29\n",
            "Train Loss for batch : 1.8533401489257812\n",
            "test_loss per batch = 1.755169153213501\n",
            "test accuracy : 0.3421\n",
            "EPOCH : 30\n",
            "Train Loss for batch : 1.8564618825912476\n",
            "test_loss per batch = 1.7883950471878052\n",
            "test accuracy : 0.3368\n",
            "EPOCH : 31\n",
            "Train Loss for batch : 1.851677656173706\n",
            "test_loss per batch = 1.7658500671386719\n",
            "test accuracy : 0.3446\n",
            "EPOCH : 32\n",
            "Train Loss for batch : 1.852736473083496\n",
            "test_loss per batch = 1.758557915687561\n",
            "test accuracy : 0.3347\n",
            "EPOCH : 33\n",
            "Train Loss for batch : 1.8496867418289185\n",
            "test_loss per batch = 1.7772740125656128\n",
            "test accuracy : 0.3345\n",
            "EPOCH : 34\n",
            "Train Loss for batch : 1.8537555932998657\n",
            "test_loss per batch = 1.7536405324935913\n",
            "test accuracy : 0.344\n",
            "EPOCH : 35\n",
            "Train Loss for batch : 1.8503141403198242\n",
            "test_loss per batch = 1.7708684206008911\n",
            "test accuracy : 0.3241\n",
            "EPOCH : 36\n",
            "Train Loss for batch : 1.8512911796569824\n",
            "test_loss per batch = 1.7895008325576782\n",
            "test accuracy : 0.3296\n",
            "EPOCH : 37\n",
            "Train Loss for batch : 1.8543916940689087\n",
            "test_loss per batch = 1.7951772212982178\n",
            "test accuracy : 0.3226\n",
            "EPOCH : 38\n",
            "Train Loss for batch : 1.8515300750732422\n",
            "test_loss per batch = 1.750325322151184\n",
            "test accuracy : 0.3478\n",
            "EPOCH : 39\n",
            "Train Loss for batch : 1.8453867435455322\n",
            "test_loss per batch = 1.7698506116867065\n",
            "test accuracy : 0.3371\n",
            "EPOCH : 40\n",
            "Train Loss for batch : 1.8467402458190918\n",
            "test_loss per batch = 1.7984530925750732\n",
            "test accuracy : 0.3294\n",
            "EPOCH : 41\n",
            "Train Loss for batch : 1.8466962575912476\n",
            "test_loss per batch = 1.7476176023483276\n",
            "test accuracy : 0.3463\n",
            "EPOCH : 42\n",
            "Train Loss for batch : 1.8506662845611572\n",
            "test_loss per batch = 1.771654725074768\n",
            "test accuracy : 0.344\n",
            "EPOCH : 43\n",
            "Train Loss for batch : 1.8512368202209473\n",
            "test_loss per batch = 1.7663651704788208\n",
            "test accuracy : 0.3443\n",
            "EPOCH : 44\n",
            "Train Loss for batch : 1.8407886028289795\n",
            "test_loss per batch = 1.7379968166351318\n",
            "test accuracy : 0.3469\n",
            "EPOCH : 45\n",
            "Train Loss for batch : 1.8500231504440308\n",
            "test_loss per batch = 1.745073914527893\n",
            "test accuracy : 0.3523\n",
            "EPOCH : 46\n",
            "Train Loss for batch : 1.8475449085235596\n",
            "test_loss per batch = 1.7638920545578003\n",
            "test accuracy : 0.3413\n",
            "EPOCH : 47\n",
            "Train Loss for batch : 1.8492673635482788\n",
            "test_loss per batch = 1.7207802534103394\n",
            "test accuracy : 0.3606\n",
            "EPOCH : 48\n",
            "Train Loss for batch : 1.8437047004699707\n",
            "test_loss per batch = 1.7647887468338013\n",
            "test accuracy : 0.3456\n",
            "EPOCH : 49\n",
            "Train Loss for batch : 1.8518610000610352\n",
            "test_loss per batch = 1.741398572921753\n",
            "test accuracy : 0.3502\n",
            "EPOCH : 50\n",
            "Train Loss for batch : 1.8511288166046143\n",
            "test_loss per batch = 1.746103286743164\n",
            "test accuracy : 0.3506\n",
            "EPOCH : 51\n",
            "Train Loss for batch : 1.8501466512680054\n",
            "test_loss per batch = 1.7916059494018555\n",
            "test accuracy : 0.3172\n",
            "EPOCH : 52\n",
            "Train Loss for batch : 1.850429654121399\n",
            "test_loss per batch = 1.7292261123657227\n",
            "test accuracy : 0.3636\n",
            "EPOCH : 53\n",
            "Train Loss for batch : 1.8458621501922607\n",
            "test_loss per batch = 1.8075737953186035\n",
            "test accuracy : 0.3268\n",
            "EPOCH : 54\n",
            "Train Loss for batch : 1.8497071266174316\n",
            "test_loss per batch = 1.7493935823440552\n",
            "test accuracy : 0.3449\n",
            "EPOCH : 55\n",
            "Train Loss for batch : 1.8512022495269775\n",
            "test_loss per batch = 1.7341301441192627\n",
            "test accuracy : 0.3506\n",
            "EPOCH : 56\n",
            "Train Loss for batch : 1.8483986854553223\n",
            "test_loss per batch = 1.7401196956634521\n",
            "test accuracy : 0.3582\n",
            "EPOCH : 57\n",
            "Train Loss for batch : 1.8509243726730347\n",
            "test_loss per batch = 1.7757906913757324\n",
            "test accuracy : 0.337\n",
            "EPOCH : 58\n",
            "Train Loss for batch : 1.844902753829956\n",
            "test_loss per batch = 1.7407528162002563\n",
            "test accuracy : 0.3459\n",
            "EPOCH : 59\n",
            "Train Loss for batch : 1.8418104648590088\n",
            "test_loss per batch = 1.7554893493652344\n",
            "test accuracy : 0.3481\n",
            "EPOCH : 60\n",
            "Train Loss for batch : 1.8504897356033325\n",
            "test_loss per batch = 1.7542433738708496\n",
            "test accuracy : 0.3516\n",
            "EPOCH : 61\n",
            "Train Loss for batch : 1.8465665578842163\n",
            "test_loss per batch = 1.7509630918502808\n",
            "test accuracy : 0.351\n",
            "EPOCH : 62\n",
            "Train Loss for batch : 1.8531086444854736\n",
            "test_loss per batch = 1.7835453748703003\n",
            "test accuracy : 0.3371\n",
            "EPOCH : 63\n",
            "Train Loss for batch : 1.845700979232788\n",
            "test_loss per batch = 1.805745244026184\n",
            "test accuracy : 0.3256\n",
            "EPOCH : 64\n",
            "Train Loss for batch : 1.8550419807434082\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-937a1ae6f1aa>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'Train Loss for batch : {train_loss/len (train_loader)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtot\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "num_batches = len (train_loader)\n",
        "\n",
        "for i in range (epochs):\n",
        "  print (f'EPOCH : {i+1}')\n",
        "\n",
        "  tot = 0\n",
        "  correct = 0\n",
        "  train_loss = 0\n",
        "  test_loss = 0\n",
        "  test_acc = 0\n",
        "  for batch, (X, y) in enumerate (train_loader):\n",
        "\n",
        "    # print (batch)\n",
        "\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    if (batch == num_batches - 1):\n",
        "      break\n",
        "\n",
        "    encoder.train()\n",
        "    # print (X.dtype)\n",
        "    X = tokeniser (X)\n",
        "    # print (X)\n",
        "    zero_tensor = torch.zeros ((batch_size, 1, token_length))\n",
        "    zero_tensor = zero_tensor.to (device)\n",
        "\n",
        "    X_appended = torch.concat ((zero_tensor, X), dim = 1)\n",
        "    # print (X)\n",
        "    X_positional_encoded = X_appended + position_encoding\n",
        "    X_post_encoder = encoder (X_positional_encoded)\n",
        "    X_post_encoder = encoder (X_positional_encoded)\n",
        "\n",
        "    # break\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn (X_post_encoder, y)\n",
        "    train_loss+=loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # break\n",
        "\n",
        "  print (f'Train Loss for batch : {train_loss/len (train_loader)}')\n",
        "  test_loss = 0\n",
        "  for batch, (X, y) in enumerate (test_loader):\n",
        "\n",
        "    tot +=len (X)\n",
        "    if (batch == len (test_loader) - 1):\n",
        "      break\n",
        "\n",
        "    encoder.eval()\n",
        "\n",
        "    X = X.to(device)\n",
        "    y = y.to (device)\n",
        "\n",
        "    X = tokeniser (X)\n",
        "    zero_tensor = torch.zeros ((batch_size, 1, token_length))\n",
        "    zero_tensor = zero_tensor.to (device)\n",
        "    X_appended = torch.concat ((zero_tensor, X), dim = 1)\n",
        "    X_positional_encoded = X_appended + position_encoding\n",
        "    X_post_encoder = encoder (X_positional_encoded)\n",
        "    X_post_encoder = encoder (X_positional_encoded)\n",
        "\n",
        "    test_loss += loss_fn (X_post_encoder, y)\n",
        "    X_post_encoder = torch.argmax ((X_post_encoder), dim = 1)\n",
        "\n",
        "    correct += (X_post_encoder == y).sum().item()\n",
        "  print (f'test_loss per batch = {test_loss/len (test_loader)}')\n",
        "  print (f'test accuracy : {correct/tot}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoDQ3r2foSNu"
      },
      "source": [
        "------------------\n",
        "#Experimentation\n",
        "------------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch, (X, y) in enumerate (test_loader):\n",
        "  print (X)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "sPh0AiTmOp45",
        "outputId": "5da8275b-ede5-4093-c494-d2eb2e3fc5c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "312",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 312",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-f280be1e5f74>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3892\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3893\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3894\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3895\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3796\u001b[0m             ):\n\u001b[1;32m   3797\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3798\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3799\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 312"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk-8TyoukY2G"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# patch_size = 50\n",
        "# unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size, padding=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCBOn8hjkVGg"
      },
      "outputs": [],
      "source": [
        "# x = torch.randn(1, 1, 100, 100)  # Batch size of 1, 1 channel, 100x100 image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ5H5n3Kkhyk"
      },
      "outputs": [],
      "source": [
        "# patches = unfold(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwSGeHa8kjX5"
      },
      "outputs": [],
      "source": [
        "# print (x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv5XJcCdkpEm"
      },
      "outputs": [],
      "source": [
        "# print (patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rp92KXEkvk7"
      },
      "outputs": [],
      "source": [
        "# layer = nn.Linear (in_features = 2500, out_features = 768)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qw8WFCll0J3"
      },
      "outputs": [],
      "source": [
        "# input = torch.randn((100, 2500))\n",
        "# print (input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDzS_B4Al8-t"
      },
      "outputs": [],
      "source": [
        "# output = layer(input)\n",
        "# print (output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbGsMUe9mDQR"
      },
      "outputs": [],
      "source": [
        "# batch, sentence_length, embedding_dim = 20, 5, 10\n",
        "# embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
        "# layer_norm = nn.LayerNorm(embedding_dim)\n",
        "# embedding_2 = layer_norm(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS7NJbJ4yIVP"
      },
      "outputs": [],
      "source": [
        "# embedding_1 = embedding[0][0]\n",
        "# mean = embedding_1.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEf5St1ZyZrq"
      },
      "outputs": [],
      "source": [
        "# var = torch.var (embedding_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eKTKE5UylAy"
      },
      "outputs": [],
      "source": [
        "# embedding_1 = (embedding_1 - mean)/(var)**1/2\n",
        "# print (embedding_1)\n",
        "# print (embedding_2[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og9GrLLZ9APU"
      },
      "source": [
        "experimentation for attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zonanV3f8-He"
      },
      "outputs": [],
      "source": [
        "# a = torch.rand (3, 4, 12)\n",
        "# b = a.reshape (3, 3, 2 , 4, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxxqSkGh89Ju"
      },
      "outputs": [],
      "source": [
        "# print (a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfHcjGJzywTv"
      },
      "outputs": [],
      "source": [
        "# print (b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOZa8xyAAs3U"
      },
      "source": [
        "checking how attention weights are calculated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLBQ9YZcArBQ"
      },
      "outputs": [],
      "source": [
        "batch_size, num_heads, num_tokens, head_dim = 2, 4, 5, 6\n",
        "\n",
        "q = torch.randint (1, 10, (batch_size, num_heads, num_tokens, head_dim))\n",
        "q = q.float()\n",
        "\n",
        "k = torch.randint (1, 10, (batch_size, num_heads, num_tokens, head_dim))\n",
        "k = k.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KHnwW06BSUI"
      },
      "outputs": [],
      "source": [
        "print (q, '\\n', k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63GJAiz-BUoT"
      },
      "outputs": [],
      "source": [
        "print (k.transpose (-2, -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFCWfHVLBgfV"
      },
      "outputs": [],
      "source": [
        "print (q.shape)\n",
        "\n",
        "weights = q@ (k.transpose (-2, -1))\n",
        "\n",
        "print (weights.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5g2swEeCPDb"
      },
      "outputs": [],
      "source": [
        "weights = weights.softmax (dim = -1)\n",
        "print (weights.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBDpoLreCbhD"
      },
      "outputs": [],
      "source": [
        "print (q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwaSx1b9D2qd"
      },
      "outputs": [],
      "source": [
        "q_modified = q.transpose (1, 2)\n",
        "print (q_modified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaaZYIk9EKAo"
      },
      "outputs": [],
      "source": [
        "q_ultra_modified = q_modified.reshape (batch_size, num_tokens, num_heads*head_dim)\n",
        "print (q_ultra_modified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHldUQ79EaGj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}